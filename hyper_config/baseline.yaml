# only name, seed, and info about data will be used to specify saved path
# it's ideal to change name for every run
name: baseline # The name of the current experiment
log_interval: 10 # Number of update steps between each logging step
# Weight & Bias
use_wandb: False # If set to True, will update the result to Weight & Bias. 
wandb_project_name: LPN_Open_Source # Project name when updating to Weight& Bias
wandb_entity_name: lpniiis # Entity name when updating to Weight& Bias
search: True # If set to true, is used to perform sample complexity search (by pyscript/lpn_hyper.py), see hyper_config/baseline for an example
# Parallelization
num_parallel: 4 # how many different seeds for initialization to test, should be a multiple of max_parallel
max_parallel: 4 # how many gpus on your computer
num_dataset: 1 # how many different dataset tested on, should be no greater than repeat
required_success_rate: 0.66 # Propotions of dataset that needs to reach the accuracy threshold
# Binary Search Range
lower_log: 20 # Lower bound on training size= 2^{lower_log / 2}
upper_log: 22 # Upper bound on training size= 2^{upper_log / 2}


data:
  seed: 0 # Random seed for data generation
  s_noise: 0.2 # Bernoulli probability of the additive noise
  s_secret: 0.2 # (Hamming weight / Dimension) for the secret
  d: 20 # Secret Dimension
  repeat: 1 # used only for generating the data, no less than num_dataset


stop_criterion:
  type: step # Specify the type of stop criterion, choose from step, time or time
  step: 1000 # Specify the maximum update step an experiment run, activated when type = step
  time: 1000 # Specify the maximum number of seconds an experiment run, activated when type = time
  acc: 0.51 # Specify the accuracy threshold of an experiment run, when reached, the experiment stopped, activated when type = acc

model:
  model_type: mlp # Specify the type of the model
  mlp:
    num_layer: 1 # Number of hidden layers in the MLP
    hidden_dim: 2000 # Widths of the intermediate layer
    activation: relu # Activation of the intermediate layer
    output_activation: sigmoid # Activation of the output layer
    dropout: 0 # Dropout ratio of the intermediate layer
  sparse_mlp:
    num_layer: 1 # Number of hidden layers in the sparse MLP
    hidden_dim: 1000 # Widths of the intermediate layer
    activation: relu # Activation of the intermediate layer
    output_activation: sigmoid # Activation of the output layer
    dropout: 0 # Dropout ratio of the intermediate layer
    sparsity: 0.2 # 1 - (#Nonzero Elements/#Elements in Weight)
  cnn:
    out_channel: 1000 # Number of Convolution Channels
    kernel_size: 3 # Kernel size for the convolution
    dropout: 0 # Dropout ratio of the intermediate layer
    activation: relu # Activation of the intermediate layer
    output_activation: sigmoid # Activation of the output layer


train:
  loss: logistic # Type of loss function, choose from mse, logistic and mae
  batch_size: 1.0 # When batch size is a fraction, fore example 1.0 in this case it corresponds to a fraction of the training set. When it is an interger, for example 2, corresponds to a deterministic batch size
  optimizer: 
    name: adam # Type of optimizer, choose from adam or sgd
    weight_decay: 0 # Weight decay parameter
    lr: 2e-4 # Learning rate
  scheduler:
    name: none # Type of learning rate scheduler, choose from none, cosine or step
  l1:
    use: False # Whether to use L1 regularization
    lamb: 0.0 # L1 regularization parameter

test:
  clean: False # When set to true, no noise is added on the test set, this shows you an approximation for the accuracy on clean sample.


